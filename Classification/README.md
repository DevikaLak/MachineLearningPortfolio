## Overview:

The goal of this project is to create a default prediction model for American Express. Complete project details can be found in the below link:
https://www.kaggle.com/competitions/amex-default-prediction/overview

The original project datasets (as seen from Data section of challenge) are huge. Training data set size of original data is 16.39 GB and contains approximately 5.53 million records. 
The original test data set size is larger i.e. 33.82 GB and contains approximately 11.4 million records.

So, a lighter version of these datasets in parquet format are obtained from the following link:
https://www.kaggle.com/datasets/raddar/amex-data-integer-dtypes-parquet-format

## Content Description:

1) Code folder: Contains all code to be executed.
2) Code/scripts: User defined scripts for training and deployment.
3) Code/Config: Required configuration for Autogluon estimator construction and training in YAML format.
4) Code/Capstone_Data_Preparation.ipynb: Code for loading parquet data from kaggle, performing EDA, Data Preprocessing and Baseline model creation.
5) Code/Autoglunon-default-pred.ipynb: Code for downloading preprocessed data from S3, training AutoGluon model in Sagemaker, deploying the model as an endpoint in sagemaker and testing the model on test data.
6) Code/Lambda.py: Code for Lambda function
7) Data: Contains the train, validation and test set after the data is preprocessed and sampled.
8) minimal_EDA.html: Report generated by Pandas profiler.
9) Proposal
10) Project Report

## Required packages:

Python 3.9
AutoGluon 0.7 
sagemaker>=2.126.0
kaggle
Pandas
Numpy
Matplotlib
ydata-profiling
sklearn

## Execution steps:

1) Execute cells in Capstone_Data_Preparation.ipynb after adding a valid kaggle API token in cell 3. If execution of any cell fails, ensure the above required packages are installed. On successful execution of all Data preprocess steps a new folder with name 'Final-Capstone-Data' will be created with a small train, test and validation set in pwd.
2) Before executing Autoglunon-default-pred.ipynb, dowloand the below mentioned helper python scripts and save them in pwd:
		-[ag_model.py](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/autogluon-tabular-containers/ag_model.py "ag_model.py")
		- [deserializers.py](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/autogluon-tabular-containers/deserializers.py "deserializers.py")
		- [sagemaker_utils.py](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/autogluon-tabular-containers/sagemaker_utils.py "sagemaker_utils.py")
		- [serializers.py](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/autogluon-tabular-containers/serializers.py "serializers.py")
3) Once done, ensure that the config-med.yaml is present under config folder and inference_script.py and training_script.py are present under scripts folder.
4) Skip the steps to download data from S3 as data is generated from step 1 and execute from cell that creates AutoGluonSagemakerEstimator object.
5) If Step 1 was not executed unzip data from Data folder provided in this zip into pwd.
6) On successful execution a trained AutoGluon predictor for the problem will be deployed as a endpoint in AWS sagemaker.
7) If Lambda function needs to be tested, use the code from Lambda.py
8) For successful execution of Lambda function ensure that sagemaker package is added as a layer to the function. Also, change the value in endpoint_name to reflect the deployed endpoint name in your environment for the predictor.
9) To test the lambda function, upload the test.csv into a S3 bucket and provide the uri with key s3_url in the test event.
		

